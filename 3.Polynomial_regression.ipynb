{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vN99YjPTDena"
   },
   "source": [
    "# Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the Dataset:**\n",
    "\n",
    "Imagine we’re part of an HR department looking to hire a promising candidate who appears to be a great fit for our company. After successfully navigating the interview process, the candidate says yes to the offer, but then comes the inevitable question: What is your salary expectation?\n",
    "\n",
    "The candidate, being highly experienced and advanced in their career, states their expectation as $160,000 per year. When we ask why they are expecting such a high salary, they respond confidently: That’s what I earned at my previous company, so I expect at least the same here.\n",
    "\n",
    "Now, as HR professionals, we need to determine whether this claim is accurate or an exaggeration. To do this, we’ll use a polynomial regression model to predict the candidate’s previous salary based on their position.\n",
    "\n",
    "**The Dataset**\n",
    "\n",
    "To make this prediction, we need data. Let’s say we collected salary information from reliable online sources like Glassdoor, showing the typical salaries for various positions in the candidate’s previous company—from Business Analyst to CEO. Additionally, by reviewing the candidate’s LinkedIn profile, we confirmed that they held the position of Region Manager for two years.\n",
    "\n",
    "In our dataset, the salary for a Region Manager is listed as $150,000$, while the next higher position (e.g., VP) earns $200,000. Since the candidate has significant experience as a Region Manager, their actual salary is likely somewhere between these two figures. For our analysis, we’ll assign their position a level of 6.5 (between levels 6 and 7).\n",
    "\n",
    "**The Goal**\n",
    "\n",
    "By training our polynomial regression model on this dataset, we will predict the salary for a position at level 6.5. Comparing this prediction to the candidate’s claimed $160,000 will allow us to determine whether their claim is truthful or a bluff.\n",
    "\n",
    "Let’s dive in and build the model to find out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZIx_naXnDyHd"
   },
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FjnmdyPLD2tS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6c8YExmOD5x5"
   },
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nQOdXhjXD_AE"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Position_Salaries.csv')\n",
    "X = dataset.iloc[:, 1:-1].values\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column is redundant, it's actually like labeling the second column, that's why we will avoid it.\n",
    "\n",
    "Then, we’re going to skip the step of splitting the dataset into a training set and a test set. The reason for this is  we want to use the entire dataset to maximise the accuracy of our model when predicting the salary for the position level between 6 and 7. By utilising all available data points, we make sure the model captures the full range of patterns and relationships within the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Le8SEL-YEOLb"
   },
   "source": [
    "## Training the Linear Regression model on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "2eZ4xxbKEcBk",
    "outputId": "41074f6d-44c7-4a04-fd49-14bda9fb2885"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rb5nWuSHEfBV"
   },
   "source": [
    "## Training the Polynomial Regression model on the whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the PolynomialFeatures class from Scikit-learn’s preprocessing module, we’ll transform the feature X into a matrix that includes X^1,X^2,…,Xn, where n is the degree of the polynomial we choose. We'll create new features by raising the position levels to higher powers (e.g., 2,3,...,n).\n",
    "\n",
    "**Transform the Feature Matrix:**\n",
    "\n",
    "We start with our current matrix of features X, which contains only one feature: the position levels (x1). We need to transform it into a new matrix of features containing:\n",
    "\n",
    "${x_1}$: the original feature.\n",
    "\n",
    "${x_1}^2$: the square of the feature (for n=2).\n",
    "\n",
    "${x_1}^3$, ${x_1}^4$: higher powers if we decide on n=3 or n=4.\n",
    "\n",
    "To achieve this, we use the fit_transform method from the PolynomialFeatures object we created earlier. This method transforms X into a new feature matrix with all polynomial combinations up to the specified degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "HYplp4pTEm0O",
    "outputId": "4c3c03dd-0def-4584-a893-aa2e72629e8f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0O8R0tzbEpvy"
   },
   "source": [
    "## Visualising the Linear Regression results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "dcTIBAEdEyve",
    "outputId": "c242f259-d9e6-442a-f026-79dffab85972"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s clear that the linear regression model is not well-suited for this dataset. While linear regression works well when the relationship between features and the target variable is approximately linear, that’s not the case here. For many position levels, the model’s predictions are significantly off from the actual salaries.\n",
    "\n",
    "For instance:\n",
    "\n",
    "At several points, the predicted salaries are far higher or lower than the real values.\n",
    "While the model fits a couple of points reasonably well, the majority of predictions deviate greatly from the actual data.\n",
    "This discrepancy demonstrates why linear regression is not ideal for this dataset. If we were to use this model to determine whether the candidate's claimed salary is truthful, it could lead to errors—such as offering a salary far higher than necessary, which would be poor negotiation strategy.\n",
    "\n",
    "This example highlights the limitations of linear regression for datasets with non-linear patterns. Next, we’ll move on to the polynomial regression model, which is far better suited for capturing the complex relationship between position levels and salaries. Let’s visualise the results to see the improvement!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "stOnSo74E52m"
   },
   "source": [
    "## Visualising the Polynomial Regression results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "UCOcurIQE7Zv",
    "outputId": "93927499-de98-4a31-a619-c373926cbe56"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue is resolved, as the predictions on the blue curve now align much more closely with the actual salaries. This improvement is achieved with just n=2. However, by increasing the polynomial degree to n=3 or n=4, the results will improve even further. Let’s demonstrate this by retraining the polynomial regression model with n=4.\n",
    "\n",
    "With n=4, the polynomial regression equation becomes:\n",
    "\n",
    "Salary=$b_0$+$b_1$×(Position Level)+$b_2$×(Position Level2)+$b_3$×(Position Level3)+$b_4$×(Position Level4)\n",
    "\n",
    "After retraining the model on the dataset, we can visualise the updated results. As expected, the polynomial regression model now fits the dataset almost perfectly. While this indicates overfitting, it is acceptable in this specific case because our goal is to achieve precise predictions for position levels between 6 and 7.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U_qsAMKnE-PJ"
   },
   "source": [
    "## Visualising the Polynomial Regression results (for higher resolution and smoother curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To further improve the visualisation, we’ll refine the curve. Previously, the graph used only the integer values of position levels (e.g., 1, 2, 3, etc.), resulting in a less smooth curve. Instead, we’ll increase the resolution by using smaller intervals (e.g., 1.0, 1.1, 1.2, etc.), creating a denser set of x-coordinates. This produces a much smoother and more aesthetically pleasing curve.\n",
    "\n",
    "The final graph illustrates a perfectly fitted polynomial regression model. While it is overfitted to this dataset, this is fine for our purpose here, as it allows for highly accurate predictions, helping us determine whether the candidate’s salary claim is truthful or a bluff.\n",
    "\n",
    "This method is mainly for demonstration purposes, as most real-world datasets involve multiple features, making such visualisations impractical. However, in this case, it beautifully highlights the power of polynomial regression. Enjoy the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "iE6EnC3fFClE",
    "outputId": "6ecb5687-3c8a-4b46-db4a-c4955c24b9de"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "diyJFZHhFFeK"
   },
   "source": [
    "## Predicting a new result with Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why Use a Two-Dimensional Array?**\n",
    "\n",
    "In Python, arrays are built using square brackets. A single pair of square brackets creates a one-dimensional list or vector. For example:\n",
    "\n",
    "[6.5] creates a list with one value, which is not the correct format for the predict method.\n",
    "To create a two-dimensional array (even if it contains only one value), we use double square brackets:\n",
    "\n",
    "[[6.5]] creates an array with one row and one column, which matches the expected format for the predict method.\n",
    "Each pair of square brackets corresponds to dimensions:\n",
    "\n",
    "The outer brackets represent rows.\n",
    "The inner brackets represent columns.\n",
    "For example:\n",
    "\n",
    "[[6.5, 5]] creates an array with one row and two columns.\n",
    "[[6.5, 5], [2, 3]] creates an array with two rows and two columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Blmp6Hn7FJW6",
    "outputId": "f01610bc-b077-4df0-cae4-ea37c8b0037f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DW7I7ZVDFNkk"
   },
   "source": [
    "## Predicting a new result with Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "uQmtnyTHFRGG",
    "outputId": "2739bf8a-6dfb-4226-b200-252ee8857097"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "polynomial_regression.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
